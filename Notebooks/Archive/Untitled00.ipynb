{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b63a8b72-3a0b-47a1-aaea-157eb3cf8516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\india\\anaconda3\\envs\\mlops\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f9370f4-7322-4394-ab3f-107884e203a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# conda environments:\n",
      "#\n",
      "base                   C:\\Users\\india\\anaconda3\n",
      "mlops                * C:\\Users\\india\\anaconda3\\envs\\mlops\n",
      "redteam                C:\\Users\\india\\anaconda3\\envs\\redteam\n",
      "resai                  C:\\Users\\india\\anaconda3\\envs\\resai\n",
      "\n",
      "C:\\Users\\india\\anaconda3\\Scripts\\jupyter.exe\n"
     ]
    }
   ],
   "source": [
    "!conda info --envs\n",
    "!where jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eba10aa-b304-4441-b8bd-526addc200ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9c12332-676b-44cb-9cae-3666a3d565c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\india\\Desktop\\Jio_Institute\\MLOps\\Project\\Nagendra\\MLOPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\india\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\india\\Desktop\\Jio_Institute\\MLOps\\Project\\Nagendra\\MLOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1afef20d-b104-4a0d-8449-2a2882545e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\india\\Desktop\\Jio_Institute\\MLOps\\Project\\Nagendra\\MLOPS\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4220e2a5-dc45-456e-8957-e318feb9cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16e488b4-4b23-445c-962b-364e1b5dd737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet_from_github(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure the request was successful\n",
    "    return pd.read_parquet(BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d9558b5-9351-49f8-8ff8-0b50643743af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define raw URLs for train, test, and prod datasets.\n",
    "train_url = \"https://raw.githubusercontent.com/Jupudy-Nagendra/MLOPS/main/Dataset/Parquet/Metro_Interstate_Traffic_Volume_train.parquet\"\n",
    "test_url  = \"https://raw.githubusercontent.com/Jupudy-Nagendra/MLOPS/main/Dataset/Parquet/Metro_Interstate_Traffic_Volume_test.parquet\"\n",
    "prod_url  = \"https://raw.githubusercontent.com/Jupudy-Nagendra/MLOPS/main/Dataset/Parquet/Metro_Interstate_Traffic_Volume_prod.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae7e6d9c-a46c-41fd-a8b9-9f29a3ca4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_parquet_from_github(train_url)\n",
    "test_df  = load_parquet_from_github(test_url)\n",
    "prod_df  = load_parquet_from_github(prod_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce4744b5-c8aa-4d16-8690-c3a5464b6114",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sort_values(by=\"date_time\")\n",
    "test_df = test_df.sort_values(by=\"date_time\")\n",
    "prod_df = prod_df.sort_values(by=\"date_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "379a0178-ea39-4658-b128-eea2826c64b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['holiday', 'temp', 'rain_1h', 'snow_1h', 'clouds_all', 'weather_main',\n",
       "       'weather_description', 'date_time', 'traffic_volume'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ab528fab-275d-45ff-98e1-5cb7797bee2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>temp</th>\n",
       "      <th>rain_1h</th>\n",
       "      <th>snow_1h</th>\n",
       "      <th>clouds_all</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>weather_description</th>\n",
       "      <th>date_time</th>\n",
       "      <th>traffic_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19662</th>\n",
       "      <td>None</td>\n",
       "      <td>289.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>overcast clouds</td>\n",
       "      <td>2012-10-02 11:00:00</td>\n",
       "      <td>4767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6094</th>\n",
       "      <td>None</td>\n",
       "      <td>290.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>overcast clouds</td>\n",
       "      <td>2012-10-02 12:00:00</td>\n",
       "      <td>5026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15731</th>\n",
       "      <td>None</td>\n",
       "      <td>291.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Clear</td>\n",
       "      <td>sky is clear</td>\n",
       "      <td>2012-10-02 14:00:00</td>\n",
       "      <td>5181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2174</th>\n",
       "      <td>None</td>\n",
       "      <td>294.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>few clouds</td>\n",
       "      <td>2012-10-02 17:00:00</td>\n",
       "      <td>5791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28172</th>\n",
       "      <td>None</td>\n",
       "      <td>293.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>few clouds</td>\n",
       "      <td>2012-10-02 18:00:00</td>\n",
       "      <td>4770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      holiday    temp  rain_1h  snow_1h  clouds_all weather_main  \\\n",
       "19662    None  289.58      0.0      0.0          90       Clouds   \n",
       "6094     None  290.13      0.0      0.0          90       Clouds   \n",
       "15731    None  291.72      0.0      0.0           1        Clear   \n",
       "2174     None  294.14      0.0      0.0          20       Clouds   \n",
       "28172    None  293.10      0.0      0.0          20       Clouds   \n",
       "\n",
       "      weather_description           date_time  traffic_volume  \n",
       "19662     overcast clouds 2012-10-02 11:00:00            4767  \n",
       "6094      overcast clouds 2012-10-02 12:00:00            5026  \n",
       "15731        sky is clear 2012-10-02 14:00:00            5181  \n",
       "2174           few clouds 2012-10-02 17:00:00            5791  \n",
       "28172          few clouds 2012-10-02 18:00:00            4770  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5d855725-7a17-423b-ab92-3b2cac021428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['holiday', 'temp', 'rain_1h', 'snow_1h', 'clouds_all', 'weather_main',\n",
       "       'weather_description', 'date_time', 'traffic_volume'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba2d320c-cb43-442c-a381-c6ed7f83aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9640db9-563c-471a-93a3-ff6906112b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateTimeExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column=\"date_time\", drop_original=True):\n",
    "        self.column = column\n",
    "        self.drop_original = drop_original\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # No fitting necessary for this transformer.\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Convert the column to datetime\n",
    "        X[self.column] = pd.to_datetime(X[self.column])\n",
    "        # Create new columns for year, month, day, and hour\n",
    "        X['year'] = X[self.column].dt.year\n",
    "        X['month'] = X[self.column].dt.month\n",
    "        X['day'] = X[self.column].dt.day\n",
    "        X['hour'] = X[self.column].dt.hour\n",
    "        # Optionally drop the original date_time column\n",
    "        if self.drop_original:\n",
    "            X = X.drop(columns=[self.column])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "08c858ac-873e-48ce-8cc5-c5132cb3802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature columns.\n",
    "numerical_cols = ['temp', 'rain_1h', 'snow_1h', 'clouds_all', 'year', 'month', 'day', 'hour']\n",
    "categorical_cols = [ 'weather_main', 'weather_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c372ff9-5b20-48d0-9a36-09b3fe796fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_pipeline = Pipeline(steps=[\n",
    "    ('datetime_extractor', DateTimeExtractor(column=\"date_time\", drop_original=True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8187b99a-2e6f-4c3c-b03c-b7630d83dc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a holiday transformer to convert the 'holiday' column to a binary feature.\n",
    "# If the value is missing or \"None\", it becomes 0; otherwise 1.\n",
    "holiday_transformer = FunctionTransformer(\n",
    "    lambda x: ((~pd.isnull(x)) & (x != \"None\")).astype(int),\n",
    "    validate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "788dcd5e-c2c4-4b09-811c-e17846d52959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the numerical transformer pipeline.\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Build the categorical transformer pipeline.\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3d0e1c5-daba-454e-bc3f-d6b389501283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all transformers in a ColumnTransformer.\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, numerical_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols),\n",
    "    ('holiday', holiday_transformer, ['holiday'])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4a33cb7-e6b8-400f-9fe3-fe8824d7e305",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = Pipeline(steps=[\n",
    "    ('datetime', datetime_pipeline),      # First extract datetime features\n",
    "    ('preprocessor', preprocessor)          # Then process numerical and categorical features\n",
    "    # You can add a model step here, e.g., ('model', SomeRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6f0e83a3-d28f-49ba-a4b4-7bd02f618c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = full_pipeline.fit_transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5906f539-3c17-42f1-9e61-b20d971accd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a4579917-1e04-4913-97bc-13b350bb98bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed_dense = X_transformed if isinstance(X_transformed, np.ndarray) else X_transformed.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1b3bc123-0107-4030-999a-e7e4d96cc906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data shape: (28922, 58)\n",
      "First row of transformed data:\n",
      "[ 0.63140899 -0.13500797 -0.02556149  1.04359682 -1.86399345  1.0234557\n",
      " -1.56852123 -0.06092323  0.          1.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          1.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Transformed data shape:\", X_transformed_dense.shape)\n",
    "print(\"First row of transformed data:\")\n",
    "print(X_transformed_dense[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9d6cc9d3-3ef9-451a-8089-772de98c2ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error retrieving feature names: Transformer holiday (type FunctionTransformer) does not provide get_feature_names_out.\n",
      "\n",
      "Transformed data shape: (28922, 58)\n",
      "First row of transformed data:\n",
      "[ 0.63140899 -0.13500797 -0.02556149  1.04359682 -1.86399345  1.0234557\n",
      " -1.56852123 -0.06092323  0.          1.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          1.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    feature_names = full_pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "    print(\"Output Feature Names:\")\n",
    "    print(feature_names)\n",
    "except Exception as e:\n",
    "    print(\"Error retrieving feature names:\", e)\n",
    "\n",
    "# For demonstration, print the shape and first row of the transformed data.\n",
    "print(\"\\nTransformed data shape:\", X_transformed_dense.shape)\n",
    "print(\"First row of transformed data:\")\n",
    "print(X_transformed_dense[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "829aacce-206b-4ed8-88cc-40b6654c7286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the full ML pipeline with a Linear Regression model.\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "182bb6d7-c982-48af-9e66-34cd0d842160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data: separate features and target.\n",
    "X_train = train_df.drop(\"traffic_volume\", axis=1)\n",
    "y_train = train_df[\"traffic_volume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4057a248-ebf2-4c8a-8a97-a0f529e68335",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A given column is not a column of the dataframe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlops\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'year'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlops\\lib\\site-packages\\sklearn\\utils\\_indexing.py:364\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[1;32m--> 364\u001b[0m     col_idx \u001b[38;5;241m=\u001b[39m \u001b[43mall_columns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col_idx, numbers\u001b[38;5;241m.\u001b[39mIntegral):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlops\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'year'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the pipeline on the training data.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mML Pipeline built and model trained successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlops\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlops\\lib\\site-packages\\sklearn\\pipeline.py:654\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    648\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    649\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    650\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    651\u001b[0m     )\n\u001b[0;32m    653\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 654\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlops\\lib\\site-packages\\sklearn\\pipeline.py:588\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m    582\u001b[0m step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    583\u001b[0m     step_idx\u001b[38;5;241m=\u001b[39mstep_idx,\n\u001b[0;32m    584\u001b[0m     step_params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[0;32m    585\u001b[0m     all_params\u001b[38;5;241m=\u001b[39mraw_params,\n\u001b[0;32m    586\u001b[0m )\n\u001b[1;32m--> 588\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlops\\lib\\site-packages\\joblib\\memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlops\\lib\\site-packages\\sklearn\\pipeline.py:1551\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1551\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1553\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   1554\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m   1555\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlops\\lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlops\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlops\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:993\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_transformers()\n\u001b[0;32m    991\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[1;32m--> 993\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_column_callables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[0;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlops\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:552\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    550\u001b[0m         columns \u001b[38;5;241m=\u001b[39m columns(X)\n\u001b[0;32m    551\u001b[0m     all_columns\u001b[38;5;241m.\u001b[39mappend(columns)\n\u001b[1;32m--> 552\u001b[0m     transformer_to_input_indices[name] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_column_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns \u001b[38;5;241m=\u001b[39m all_columns\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_to_input_indices \u001b[38;5;241m=\u001b[39m transformer_to_input_indices\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlops\\lib\\site-packages\\sklearn\\utils\\_indexing.py:372\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    369\u001b[0m         column_indices\u001b[38;5;241m.\u001b[39mappend(col_idx)\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA given column is not a column of the dataframe\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m column_indices\n",
      "\u001b[1;31mValueError\u001b[0m: A given column is not a column of the dataframe"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline on the training data.\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "print(\"ML Pipeline built and model trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "79eef464-c293-44e1-b449-9d85660640ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "# Prepare test set features and labels.\n",
    "X_test = test_df.drop(\"traffic_volume\", axis=1)\n",
    "y_test = test_df[\"traffic_volume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "5d05e339-81c7-4b8e-b701-c51831d4c8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics on Test Data:\n",
      "Root Mean Squared Error (RMSE): 5623.18\n",
      "R-squared (R2 Score): -6.94\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# Get predictions from the model.\n",
    "predictions = model_pipeline.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error, then take the square root for RMSE.\n",
    "mse = metrics.mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Calculate R-squared (R2 score).\n",
    "r2 = metrics.r2_score(y_test, predictions)\n",
    "\n",
    "print(\"Evaluation Metrics on Test Data:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R-squared (R2 Score): {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "74becedd-56a8-427e-a24f-9a6614e77048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "f57faff3-d7a4-44c7-bbf2-7a3a5decc0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Feature Names:\n",
      "['num__temp' 'num__rain_1h' 'num__snow_1h' 'num__clouds_all'\n",
      " 'cat__weather_main_Clear' 'cat__weather_main_Clouds'\n",
      " 'cat__weather_main_Drizzle' 'cat__weather_main_Fog'\n",
      " 'cat__weather_main_Haze' 'cat__weather_main_Mist'\n",
      " 'cat__weather_main_Rain' 'cat__weather_main_Smoke'\n",
      " 'cat__weather_main_Snow' 'cat__weather_main_Squall'\n",
      " 'cat__weather_main_Thunderstorm' 'cat__weather_description_SQUALLS'\n",
      " 'cat__weather_description_Sky is Clear'\n",
      " 'cat__weather_description_broken clouds'\n",
      " 'cat__weather_description_drizzle' 'cat__weather_description_few clouds'\n",
      " 'cat__weather_description_fog' 'cat__weather_description_freezing rain'\n",
      " 'cat__weather_description_haze'\n",
      " 'cat__weather_description_heavy intensity drizzle'\n",
      " 'cat__weather_description_heavy intensity rain'\n",
      " 'cat__weather_description_heavy snow'\n",
      " 'cat__weather_description_light intensity drizzle'\n",
      " 'cat__weather_description_light intensity shower rain'\n",
      " 'cat__weather_description_light rain'\n",
      " 'cat__weather_description_light rain and snow'\n",
      " 'cat__weather_description_light shower snow'\n",
      " 'cat__weather_description_light snow' 'cat__weather_description_mist'\n",
      " 'cat__weather_description_moderate rain'\n",
      " 'cat__weather_description_overcast clouds'\n",
      " 'cat__weather_description_proximity shower rain'\n",
      " 'cat__weather_description_proximity thunderstorm'\n",
      " 'cat__weather_description_proximity thunderstorm with drizzle'\n",
      " 'cat__weather_description_proximity thunderstorm with rain'\n",
      " 'cat__weather_description_scattered clouds'\n",
      " 'cat__weather_description_shower drizzle'\n",
      " 'cat__weather_description_shower snow'\n",
      " 'cat__weather_description_sky is clear' 'cat__weather_description_sleet'\n",
      " 'cat__weather_description_smoke' 'cat__weather_description_snow'\n",
      " 'cat__weather_description_thunderstorm'\n",
      " 'cat__weather_description_thunderstorm with drizzle'\n",
      " 'cat__weather_description_thunderstorm with heavy rain'\n",
      " 'cat__weather_description_thunderstorm with light drizzle'\n",
      " 'cat__weather_description_thunderstorm with light rain'\n",
      " 'cat__weather_description_thunderstorm with rain'\n",
      " 'cat__weather_description_very heavy rain' 'holiday__holiday']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "\n",
    "class HolidayBinaryTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Assumes X is a DataFrame with one column 'holiday'\n",
    "        # Convert holiday values to binary: 0 if null or \"None\", else 1.\n",
    "        return ((~pd.isnull(X)) & (X != \"None\")).astype(int)\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        # Return a single feature name for the holiday column.\n",
    "        return ['holiday']\n",
    "\n",
    "# Example usage with ColumnTransformer:\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Example feature columns (assuming X_train is your DataFrame)\n",
    "numerical_cols = ['temp', 'rain_1h', 'snow_1h', 'clouds_all']\n",
    "categorical_cols = ['weather_main', 'weather_description']\n",
    "\n",
    "# Build numerical transformer pipeline.\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "# Build categorical transformer pipeline.\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Use the custom HolidayBinaryTransformer for the holiday column.\n",
    "holiday_transformer = HolidayBinaryTransformer()\n",
    "\n",
    "# Combine all transformers.\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, numerical_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols),\n",
    "    ('holiday', holiday_transformer, ['holiday'])\n",
    "])\n",
    "\n",
    "# Fit the preprocessor with training data (assuming X_train exists).\n",
    "# For demonstration, let's assume X_train is defined:\n",
    "# X_train = your DataFrame with all columns including 'holiday'\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Now, get the output feature names.\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "print(\"Transformed Feature Names:\")\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a0e46249-f693-48b4-9376-bf1aad267cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train columns: ['holiday', 'temp', 'rain_1h', 'snow_1h', 'clouds_all', 'weather_main', 'weather_description', 'date_time']\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train columns:\", X_train.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b8793827-04e7-4212-bf3d-4e3bf386bf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical output shape: (28922, 4)\n",
      "Categorical output shape: (28922, 49)\n",
      "Holiday output shape: (28922, 1)\n"
     ]
    }
   ],
   "source": [
    "# Numerical features\n",
    "num_out = numerical_transformer.fit_transform(X_train[numerical_cols])\n",
    "print(\"Numerical output shape:\", num_out.shape)\n",
    "\n",
    "# Categorical features\n",
    "cat_out = categorical_transformer.fit_transform(X_train[categorical_cols])\n",
    "print(\"Categorical output shape:\", cat_out.shape)\n",
    "\n",
    "# Holiday feature (using the custom transformer)\n",
    "holiday_out = holiday_transformer.fit_transform(X_train[['holiday']])\n",
    "print(\"Holiday output shape:\", holiday_out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "a216826e-248a-4b52-837d-3fce4a23a7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined transformed shape: (28922, 54)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Transform each subset\n",
    "num_transformed = numerical_transformer.transform(X_train[numerical_cols])\n",
    "cat_transformed = categorical_transformer.transform(X_train[categorical_cols])\n",
    "holiday_transformed = holiday_transformer.transform(X_train[['holiday']])\n",
    "\n",
    "# If the categorical output is a sparse matrix, convert it to a dense array\n",
    "if hasattr(cat_transformed, \"toarray\"):\n",
    "    cat_transformed = cat_transformed.toarray()\n",
    "\n",
    "# Combine all outputs horizontally\n",
    "combined = np.hstack([num_transformed, cat_transformed, holiday_transformed])\n",
    "print(\"Combined transformed shape:\", combined.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "17cff8fb-6364-4b6a-9d48-e2351d0f88b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of feature names: 54\n"
     ]
    }
   ],
   "source": [
    "# For numerical, we can simply use the column names.\n",
    "num_feature_names = numerical_cols\n",
    "\n",
    "# For categorical, OneHotEncoder provides get_feature_names_out\n",
    "cat_feature_names = categorical_transformer.named_steps['onehot'].get_feature_names_out(categorical_cols)\n",
    "\n",
    "# For holiday, our custom transformer returns a fixed name.\n",
    "holiday_feature_names = holiday_transformer.get_feature_names_out()\n",
    "\n",
    "# Combine the feature names\n",
    "all_feature_names = list(num_feature_names) + list(cat_feature_names) + list(holiday_feature_names)\n",
    "print(\"Total number of feature names:\", len(all_feature_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "7f21f077-5245-44b1-a465-4b99976c23f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined transformed shape: (28922, 54)\n"
     ]
    }
   ],
   "source": [
    "print(\"Combined transformed shape:\", combined.shape)  # Expect (n_samples, len(all_feature_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "95e45853-13a1-4fc5-8925-4868bb28604c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "1cc9b45c-b61f-4de0-8af7-09bfd820a2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row of transformed data:\n",
      "[ 0.63140899 -0.13500797 -0.02556149  1.04359682  0.          1.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          1.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'combined' is your NumPy ndarray\n",
    "print(\"First row of transformed data:\")\n",
    "print(combined[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "a78919f5-3f11-45ef-94e4-63cde40c2e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row with columns:\n",
      "       temp   rain_1h   snow_1h  clouds_all  weather_main_Clear  \\\n",
      "0  0.631409 -0.135008 -0.025561    1.043597                 0.0   \n",
      "\n",
      "   weather_main_Clouds  weather_main_Drizzle  weather_main_Fog  \\\n",
      "0                  1.0                   0.0               0.0   \n",
      "\n",
      "   weather_main_Haze  weather_main_Mist  ...  weather_description_smoke  \\\n",
      "0                0.0                0.0  ...                        0.0   \n",
      "\n",
      "   weather_description_snow  weather_description_thunderstorm  \\\n",
      "0                       0.0                               0.0   \n",
      "\n",
      "   weather_description_thunderstorm with drizzle  \\\n",
      "0                                            0.0   \n",
      "\n",
      "   weather_description_thunderstorm with heavy rain  \\\n",
      "0                                               0.0   \n",
      "\n",
      "   weather_description_thunderstorm with light drizzle  \\\n",
      "0                                                0.0     \n",
      "\n",
      "   weather_description_thunderstorm with light rain  \\\n",
      "0                                               0.0   \n",
      "\n",
      "   weather_description_thunderstorm with rain  \\\n",
      "0                                         0.0   \n",
      "\n",
      "   weather_description_very heavy rain  holiday  \n",
      "0                                  0.0      0.0  \n",
      "\n",
      "[1 rows x 54 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assume 'combined' is your NumPy array from the transformed data,\n",
    "# and 'all_feature_names' is a list of column names corresponding to each feature.\n",
    "# For demonstration, let's print the first row.\n",
    "\n",
    "row_df = pd.DataFrame([combined[0]], columns=all_feature_names)\n",
    "print(\"First row with columns:\")\n",
    "print(row_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "17ff195d-d823-4ff6-aa05-6e58fbb0b2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holiday column value counts:\n",
      "Value 0: 28879\n",
      "Value 1: 43\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming 'combined' is your transformed NumPy array and the holiday column is the last column:\n",
    "holiday_col = combined[:, -1]\n",
    "\n",
    "# Use np.unique to count unique values and their frequencies.\n",
    "unique, counts = np.unique(holiday_col, return_counts=True)\n",
    "\n",
    "# Print the results.\n",
    "print(\"Holiday column value counts:\")\n",
    "for value, count in zip(unique, counts):\n",
    "    print(f\"Value {int(value)}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "4768619c-25c6-4fe9-96e8-e03947399d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.4-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\india\\anaconda3\\envs\\mlops\\lib\\site-packages (from xgboost) (2.0.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\india\\anaconda3\\envs\\mlops\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-2.1.4-py3-none-win_amd64.whl (124.9 MB)\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 6.8/124.9 MB 38.1 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 11.5/124.9 MB 30.1 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 14.7/124.9 MB 24.3 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 17.8/124.9 MB 21.6 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 21.5/124.9 MB 20.9 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 25.2/124.9 MB 20.7 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 26.5/124.9 MB 18.4 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 29.4/124.9 MB 17.9 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 31.2/124.9 MB 16.9 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 33.3/124.9 MB 16.3 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 36.2/124.9 MB 15.9 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 38.5/124.9 MB 15.5 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 40.6/124.9 MB 15.2 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 43.0/124.9 MB 14.9 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 44.3/124.9 MB 14.4 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 45.6/124.9 MB 13.8 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 47.2/124.9 MB 13.5 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 48.8/124.9 MB 13.2 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 50.1/124.9 MB 12.8 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 51.6/124.9 MB 12.6 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 53.5/124.9 MB 12.4 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 55.6/124.9 MB 12.3 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 57.7/124.9 MB 12.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 59.8/124.9 MB 12.1 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 62.4/124.9 MB 12.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 65.3/124.9 MB 12.2 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 68.2/124.9 MB 12.2 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 71.3/124.9 MB 12.4 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 73.9/124.9 MB 12.4 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 76.3/124.9 MB 12.4 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 77.6/124.9 MB 12.3 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 78.9/124.9 MB 12.0 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 80.2/124.9 MB 11.8 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 81.8/124.9 MB 11.7 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 82.6/124.9 MB 11.7 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 84.4/124.9 MB 11.4 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 87.3/124.9 MB 11.5 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 89.9/124.9 MB 11.5 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 92.0/124.9 MB 11.5 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 94.4/124.9 MB 11.5 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 96.5/124.9 MB 11.4 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 99.6/124.9 MB 11.5 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 103.3/124.9 MB 11.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 107.0/124.9 MB 11.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 109.8/124.9 MB 11.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 112.5/124.9 MB 11.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 114.8/124.9 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 116.4/124.9 MB 11.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 119.0/124.9 MB 11.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 121.4/124.9 MB 11.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  123.5/124.9 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.8/124.9 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.8/124.9 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------------------------------- 124.9/124.9 MB 11.4 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00b723f-a6f2-4a52-9db8-8dbb55247ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlops)",
   "language": "python",
   "name": "mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
